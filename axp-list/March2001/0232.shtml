<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN"> 
     <HTML> 
     <HEAD> 
     <TITLE>Axp-List Archive</TITLE> 
     <LINK REV="made" HREF="mailto:mailto-address"> 
     <HEAD> 
     <BODY BGCOLOR="#DC9D33" TEXT="#000000" LINK="#DD0000" ALINK="#CC0000" VLINK="#CC0000">
		<CENTER>  <!--#exec cgi="/cgi-bin/banmat1.cgi"--></CENTER>

     <H1 ALIGN=CENTER>Axp-List Archive<BR> RE: Interesting SCSI Performance Problems</H1> 
	
<!-- received="Fri Mar 16 22:57:02 2001" -->
<!-- isoreceived="20010317065702" -->
<!-- sent="Fri, 16 Mar 2001 14:36:22 -0800" -->
<!-- isosent="20010316223622" -->
<!-- name="KirkE@paccessglobal.com" -->
<!-- email="KirkE@paccessglobal.com" -->
<!-- subject="RE: Interesting SCSI Performance Problems" -->
<!-- id="4A3D99071F51D411934C0008C7C9DEDD1579D2@pdxexch.paccessglobal.net" -->
<!-- inreplyto="Interesting SCSI Performance Problems" -->
<STRONG>Subject: </STRONG>RE: Interesting SCSI Performance Problems<BR>
<EM>KirkE@paccessglobal.com</EM><BR>
<STRONG>Date: </STRONG>Fri Mar 16 14:36:22 2001
<P>
<UL>
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.shtml#232">[ date ]</A>
<A HREF="index.shtml#232">[ thread ]</A>
<A HREF="subject.shtml#232">[ subject ]</A>
<A HREF="author.shtml#232">[ author ]</A>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="0233.shtml">Andrei A. Dergatchev: "Re: Interesting SCSI Performance Problems - discussions where ?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0231.shtml">Andrei A. Dergatchev: "Re: SCSI HD trubble"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="0227.shtml">T. Daniel Crawford: "Interesting SCSI Performance Problems"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0233.shtml">Andrei A. Dergatchev: "Re: Interesting SCSI Performance Problems - discussions where ?"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="0227.shtml">KirkE@paccessglobal.com: "RE: Interesting SCSI Performance Problems"</A>
<!-- reply="end" -->
</UL>
<HR>
<!-- body="start" -->
<P>
Daniel,
<BR>
<P><EM>&gt;On each of my workstations I have three 18 GB Ultra 160 SCSI disks, but,
</EM><BR>
<EM>&gt;unfortunately the on-board host adapter is a Qlogic ISP1020 Ultra Wide SCSI
</EM><BR>
<EM>&gt;that is capable of only 40 MB/s transfer rates.  
</EM><BR>
<P>Its actually a QLA 1040B, the 1020 is a Fast-20 HBA (Fast SCSI II wide).
<BR>
The last number in Qlogic part number denotes the rate of the ASIC.  The QLA
<BR>
1080 is Ultra 2 (Fast 40) and the QLA 10160 is Ultra 160 (Fast 80).  The QLA
<BR>
1040B is an SRM compatible boot device, but shouldn't be used for data
<BR>
volumes.  I would advise the following:
<BR>
<P>1) First SCSI controller (1040B) single boot disk (9GB Ultra disks can be
<BR>
had for $80.00 on Pricewatch.com)
<BR>
2) Second SCSI controller (10160 or 12160, 12160 being dual channel) with
<BR>
2-4 disks (may be software stripped, if desired).
<BR>
<P>The Qlogic 1240 (dual channel 1040B), 1080, 1280, 10160, 12160 controllers
<BR>
are all 64 bit PCI, boosting your max throughout for the controller (not
<BR>
necessarily the disks) up to 180-200MBps (minus some bus overhead). You
<BR>
could easily stack 3 disks per channel on a 1080/10160 for fairly
<BR>
spectacular performance.  The more you know about the behavior of your app
<BR>
with regard to disk I/O requirements, the better.  An NVRAM dump utility is
<BR>
provided that will allow you to modify the NVRAM parameters (you'll have to
<BR>
use a DOS machine, Win98/ME, etc to run the tool) and tune the FIFO
<BR>
thresholds, bus timings, etc.  Its worth the extra effort.
<BR>
<P>Alternatively, you could use a Myles RAID controller (such as the
<BR>
eXtremeRAID 2000) and run one or more logical volumes in hardware RAID &quot;10&quot;
<BR>
(mirrored stripes) for the ideal mix between random uncached reads/writes
<BR>
and sequential cached reads/writes (its the fastest all around RAID level,
<BR>
though at 50% capacity overhead).  I noticed that you had some negotiation
<BR>
problems with your disks, possibly due to spin up/spin down during
<BR>
negotiation.  Unless you have a good reason to maintain the disk spinup at
<BR>
boot, disable the feature.  I would recommend that for your purposes you
<BR>
consider an external drive array with two SCSI busses (3 or 4 disks per
<BR>
bus), these can be had on Pricewatch, Egghead auctions and Ebay for between
<BR>
$200.00 and $1,000.00 and usually contain one or two 350-550 watt power
<BR>
supplies, reducing the load on your XP1000's often over-demanded power
<BR>
supply.
<BR>
<P>As for your synthetic load generator program to determine max throughput, be
<BR>
aware that unless your application writes or reads in sequential fashion,
<BR>
you will not achieve anywhere NEAR those figures (34-74MBps is based on
<BR>
synchronous rates at the highest negotiated frequency and with highly
<BR>
cacheable sequentially read/written data). Not even a caching RAID
<BR>
controller like the eXtremeRAID 1100 or 2000 will achieve maximum throughput
<BR>
with applications that require paging of only small amounts of data at a
<BR>
time (high I/O throughput rate, not high MBps rates due to high overhead,
<BR>
they are mutually exclusive).  Most of the tricks a good caching RAID
<BR>
controller will due are in late-write and read-ahead, neither of which are
<BR>
possible with highly random data requests. A realistic data rate will fall
<BR>
somewhere between maximum throughput in MBps and the minimum MBps for a
<BR>
large number of small I/O transactions (perhaps close to a thousand per
<BR>
second). Your results will be application dependent.
<BR>
<P>For highly transactional (such as DB) I/O requests, it is better to shrink
<BR>
the RAID stripe blocks down to a fairly small number (8KB to 16KB), this may
<BR>
vary depending on the DB native paging size and the hardware platform. Cache
<BR>
should be write-back to provide delayed committal to disk.  Read-ahead won't
<BR>
benefit you much.
<BR>
<P>For streaming (such as video editing, DB logs) I/O requests, is better to
<BR>
use a large or very large stripe block (64KB-256KB), this will vary
<BR>
depending on the average file size, the transaction commit size and the
<BR>
capabilities of the RAID controller.  Write-back should be set (where
<BR>
possible) to the longest delay before a cache buffer overflow.  Read-ahead
<BR>
will have substantial benefit in sequetentially read data. 
<BR>
<P><EM>&gt;I wrote a little test program to simply write a 2.4 GB file to disk,
</EM><BR>
including the appropriate fdatasync() to isolate 
<BR>
<EM>&gt;hardware problems from buffering.  My timing with the ISP1020 controller: a
</EM><BR>
whopping 9 MB/s sustained transfer rate.
<BR>
<P>As I mentioned, this is a 1040B, NOT a 1020 (two controllers that are VERY
<BR>
different and more than 3 years of ASIC development between them).  This
<BR>
sounds about right for an average sustained throughput for that controller,
<BR>
at least for reads that cannot be effectively buffered with a single disk.
<BR>
You may be able to tweak it for your I/O access type through the NVRAM
<BR>
tweaks I mentioned above, though I encourage you to look at a secondary data
<BR>
only controller or a RAID controller for the same purpose.  Budget and value
<BR>
to your development may be the deciding factor.
<BR>
<P><EM>&gt;I then looked through the qlogicisp.c driver code in the 2.4.2 kernel and
</EM><BR>
played
<BR>
<EM>&gt;with default settings.  I found that if I recompiled the code to grab its
</EM><BR>
<EM>&gt;defaults from the card's NVRAM, then I could get a 34 MB/s transfer rate
</EM><BR>
<EM>&gt;out of the Qlogic card, but if I used the compiled-in defaults, I could
</EM><BR>
<EM>&gt;get 17 MB/s at best.
</EM><BR>
<P>Not too surprising, many of the SCSI controller drivers I've seen do NOT
<BR>
reference the NVRAM variables which will not use more of the advanced
<BR>
features and a higher frequency bus during or after negotiation.  These
<BR>
drivers are compiled for safety, not speed. As mentioned, the NVRAM can be
<BR>
further tweaked, just not from your Alpha (NVRAM dump utils from Qlogic's
<BR>
website are readmode DOS only). 
<BR>
<P>Good luck,
<BR>
<P>--KE 
<BR>
<P><P><P>_______________________________________________
<BR>
Axp-list mailing list
<BR>
Axp-list@redhat.com
<BR>
https://listman.redhat.com/mailman/listinfo/axp-list
<BR>
<P><!-- body="end" -->
<HR>
<P>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="0233.shtml">Andrei A. Dergatchev: "Re: Interesting SCSI Performance Problems - discussions where ?"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0231.shtml">Andrei A. Dergatchev: "Re: SCSI HD trubble"</A>
<LI><STRONG>Maybe in reply to:</STRONG> <A HREF="0227.shtml">T. Daniel Crawford: "Interesting SCSI Performance Problems"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0233.shtml">Andrei A. Dergatchev: "Re: Interesting SCSI Performance Problems - discussions where ?"</A>
<LI><STRONG>Maybe reply:</STRONG> <A HREF="0227.shtml">KirkE@paccessglobal.com: "RE: Interesting SCSI Performance Problems"</A>
<!-- reply="end" -->
</UL>
<!-- trailer="footer" -->
<HR> 
     <P> 
     <SMALL> 
     <EM> 
     This archive was generated by  <A HREF="http://www.landfield.com/hypermail">hypermail version 2a22 </A> on Tue Apr 10 10:35:32 2001 PDT <BR>
	Send any problems or questions about this archive to <A HREF="mailto:webmaster@alphalinux.org">webmaster@alphalinux.org</A>. 
     </EM> 
     </SMALL> 
     </BODY> 
     </HTML> 
