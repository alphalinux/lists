<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML>
<HEAD>
<TITLE>Linux Alpha List: Re: 128-node Alpha 21264 cluster?</TITLE>
<META NAME="Author" CONTENT="Robert G. Brown (rgb@phy.duke.edu)">
<META NAME="Subject" CONTENT="Re: 128-node Alpha 21264 cluster?">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1 ALIGN=CENTER>Re: 128-node Alpha 21264 cluster?</H1>
<HR>
<P>
<!-- received="Sun Apr 04 05:32:46 1999 PST" -->
<!-- sent="Sun, 4 Apr 1999 00:30:36 -0500 (EST)" -->
<!-- name="Robert G. Brown" -->
<!-- email="rgb@phy.duke.edu" -->
<!-- subject="Re: 128-node Alpha 21264 cluster?" -->
<!-- id="Pine.LNX.3.96.990403234320.11684C-100000@ganesh.phy.duke.edu" -->
<!-- inreplyto="99040315110800.03307@pc2.myhouse" -->
<STRONG>Robert G. Brown</STRONG> (<A HREF="mailto:rgb@phy.duke.edu?subject=Re:%20128-node%20Alpha%2021264%20cluster?"><EM>rgb@phy.duke.edu</EM></A>)<BR>
<EM>Sun, 4 Apr 1999 00:30:36 -0500 (EST)</EM>
<P>
<UL>
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#19">[ date ]</A>
<A HREF="index.html#19">[ thread ]</A>
<A HREF="subject.html#19">[ subject ]</A>
<A HREF="author.html#19">[ author ]</A>
<!-- next="start" -->
<LI><STRONG>Previous message:</STRONG> <A HREF="0018.html">Casioqv: "Re: 128-node Alpha 21264 cluster?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0016.html">uwe@uwix.alt.na: "Re: 128-node Alpha 21264 cluster?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0003.html">Steven Castillo: "Re: 128-node Alpha 21264 cluster?"</A>
</UL>
<HR>
<!-- body="start" -->
<P>
On Sat, 3 Apr 1999, Casioqv wrote:
<BR>
<P><EM>&gt; It is, I find complete systems in the newspaper for that much.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; On Sat, 03 Apr 1999, <A HREF="mailto:uwe@uwix.alt.na?subject=Re:%20128-node%20Alpha%2021264%20cluster?">uwe@uwix.alt.na</A> wrote:
</EM><BR>
<EM>&gt; &gt;On Thu, 1 Apr 1999, Casioqv wrote:
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<EM>&gt; &gt;&gt; I find that 486 100's are currently the most cost effective. At about $50/box
</EM><BR>
<EM>&gt; &gt;&gt; that is $500/ghz + networking.
</EM><BR>
<EM>&gt; &gt;
</EM><BR>
<P>Gee, I thought this was an April Fool's joke but it keeps going;-).
<BR>
<P>Seriously now...
<BR>
<P>&nbsp;&nbsp;a) Used 486 systems may be of use to a very few persons, mostly for
<BR>
&quot;having fun and learning&quot;, but they aren't generally available and
<BR>
aren't generally available for $50 where they are available, and aren't
<BR>
a very good choice for a serious numerical computation.
<BR>
<P>&nbsp;&nbsp;b) Raw clock scaling is not a good predictor of performance across
<BR>
processor generations.  The 486 (P4) is two to three generations behind
<BR>
current P6's (Celerons, PII's and PIII's).  A 400 MHz Celeron system
<BR>
probably has as much raw speed (especially in floating point) as all ten
<BR>
486's put together (that's just 2.5x clock speed scaling over 2.5
<BR>
processor generations) and certainly costs less.  A &quot;GHz&quot; of CPU
<BR>
performance is as meaningless as a &quot;bogogip&quot; (the same thing, actually).
<BR>
What matters is how long it takes one's (often floating point intensive)
<BR>
code to finish.
<BR>
<P>&nbsp;&nbsp;c) In addition to much better floating point performance, P6-family
<BR>
processors have better caches (speed and size) and sit on a faster
<BR>
memory bus accessing a better memory design (SDRAM).  I would bet that a
<BR>
486, even at 100 MHz, is not fast enough to push a 100BT NIC at
<BR>
capacity, for example.  There was evidence of this in the netperf
<BR>
database, if I recall correctly.
<BR>
<P>&nbsp;&nbsp;d) The networking cost is non-trivial.  Let's be generous and assume
<BR>
$20/NIC plus $50/switched port.  That's another $700.  Heck, make it
<BR>
just $500.  That makes the true ten-node &quot;system&quot; cost $1000 or more.
<BR>
<P>For $1000, one can build TWO 400 MHz Celeron systems with 64 MB of SDRAM
<BR>
each, your choice of a 3 GB HD each or an 8 GB in one to serve the
<BR>
other, and a small stack of 100BT NIC's.  No need for a switch with only
<BR>
two nodes.  I'd bet that on nearly any task, either one of them would
<BR>
tie or beat all ten 486's working on embarrassingly parallel code.
<BR>
<P>Of course it's downhill from embarrassingly parallel code that fits in
<BR>
the presumed tiny memories of the $50 486's for the ten boxes...and one
<BR>
probably has an extra $200-500 to spend for a proper comparison since,
<BR>
as Uwe also noted, there are a lot more costs to consider for a ten node
<BR>
system than for just one, two, or three.  Our estimated ten-system
<BR>
network cost is heavily lowball at $500 to begin with.  Then one needs
<BR>
five times as much electricity and five times as much cooling and five
<BR>
times as much shelf or floor space and five times as much miscellaneous
<BR>
&quot;stuff&quot;.  Ten 7' RJ45 cables add up to another $70 or so (over the
<BR>
counter price from a CHEAP place -- more like $150 from Best Buy), for
<BR>
example, and then there are a couple of $20 power strips required and it
<BR>
takes more human time to set up and run all those systems and (being
<BR>
old) a ten node 486 system is going to be MUCH less stable than a one or
<BR>
two or three node systems made with brand new 400 MHz Celerons, and
<BR>
then...
<BR>
<P>I think that if one looked at REAL cost, three 400 MHz diskless Celeron
<BR>
nodes could be built and directly interconnected for what it costs to
<BR>
buy 12 486's and network them together, even at $50/each for the 486
<BR>
systems.  The new Celeron SMP systems drop the comparative marginal cost
<BR>
per CPU-Hz even more.  I cannot imagine that a three node 400 MHz
<BR>
Celeron beowulf wouldn't outperform the 12 node 486 beowulf by a factor
<BR>
of two or more and would be MUCH easier to program (it would yield most
<BR>
of the speed advantage for ordinary von Neumann code).
<BR>
<P>Given the choice, I know which one &gt;&gt;I'd&lt;&lt; take...
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;rgb
<BR>
<P>Robert G. Brown	                       <A HREF="http://www.phy.duke.edu/~rgb/">http://www.phy.duke.edu/~rgb/</A>
<BR>
Duke University Dept. of Physics, Box 90305
<BR>
Durham, N.C. 27708-0305
<BR>
Phone: 1-919-660-2567  Fax: 919-660-2525     email:<A HREF="mailto:rgb@phy.duke.edu?subject=Re:%20128-node%20Alpha%2021264%20cluster?">rgb@phy.duke.edu</A>
<BR>
<P><!-- body="end" -->
<HR>
<P>
<UL>
<!-- next="start" -->
<LI><STRONG>Previous message:</STRONG> <A HREF="0018.html">Casioqv: "Re: 128-node Alpha 21264 cluster?"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0016.html">uwe@uwix.alt.na: "Re: 128-node Alpha 21264 cluster?"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0003.html">Steven Castillo: "Re: 128-node Alpha 21264 cluster?"</A>
</UL>
<!-- trailer="footer" -->
<HR>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.landfield.com/hypermail/">hypermail 2.0b3</A> 
on <EM>Sat Apr 03 1999 - 22:00:07 PST</EM>
</EM>
</SMALL>
</BODY>
</HTML>
