<!DOCTYPE HTML PUBLIC "-//IETF//DTD HTML//EN">
<HTML>
<HEAD>
<TITLE>Linux Alpha List: Re: new IRQ scalability changes in 2.3.48</TITLE>
<META NAME="Author" CONTENT="Ingo Molnar (mingo@chiara.csoma.elte.hu)">
<META NAME="Subject" CONTENT="Re: new IRQ scalability changes in 2.3.48">
</HEAD>
<BODY BGCOLOR="#FFFFFF" TEXT="#000000">
<H1 ALIGN=CENTER>Re: new IRQ scalability changes in 2.3.48</H1>
<HR>
<P>
<!-- received="Wed Mar 15 01:57:59 2000" -->
<!-- isoreceived="20000315095759" -->
<!-- sent="Tue, 14 Mar 2000 21:54:14 +0100 (CET)" -->
<!-- isosent="20000314205414" -->
<!-- name="Ingo Molnar" -->
<!-- email="mingo@chiara.csoma.elte.hu" -->
<!-- subject="Re: new IRQ scalability changes in 2.3.48" -->
<!-- id="Pine.LNX.4.10.10003142134540.6853-100000@chiara.csoma.elte.hu" -->
<!-- inreplyto="20000314122419.A20431@hq.fsmlabs.com" -->
<STRONG>Subject: </STRONG>Re: new IRQ scalability changes in 2.3.48<BR>
<STRONG>From: </STRONG>Ingo Molnar (<EM>mingo@chiara.csoma.elte.hu</EM>)<BR>
<STRONG>Date: </STRONG>Tue Mar 14 2000 - 12:54:14 PST
<P>
<UL>
<LI><STRONG>Messages sorted by:</STRONG> 
<A HREF="date.html#79">[ date ]</A>
<A HREF="index.html#79">[ thread ]</A>
<A HREF="subject.html#79">[ subject ]</A>
<A HREF="author.html#79">[ author ]</A>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="0080.html">Andrea Arcangeli: "Re: [patch] preemptive kernel, preemptive-2.3.52-A7"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0078.html">Jamie Lokier: "Re: new IRQ scalability changes in 2.3.48"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0098.html">yodaiken@fsmlabs.com: "Re: new IRQ scalability changes in 2.3.48"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0091.html">yodaiken@fsmlabs.com: "Re: new IRQ scalability changes in 2.3.48"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0098.html">Ingo Molnar: "Re: new IRQ scalability changes in 2.3.48"</A>
<!-- reply="end" -->
</UL>
<HR>
<!-- body="start" -->
<P>
On Tue, 14 Mar 2000 yodaiken@fsmlabs.com wrote:
<BR>
<P><EM>&gt; &gt; people will have the nicely preemptable UP kernel. The latency-quality of
</EM><BR>
<EM>&gt; &gt; the SMP kernel will be much worse than the UP kernel's latency.
</EM><BR>
<EM>&gt; 
</EM><BR>
<EM>&gt; I don't see why. Can you explain? 
</EM><BR>
<P>yep: the SMP kernel will not be preemptible like the UP kernel.
<BR>
<P><EM>&gt; Why ? The length of time between scheduler calls under SMP should be 
</EM><BR>
<EM>&gt; smaller than the length of time between scheduler calls on UP - no?
</EM><BR>
<P>no, they would be bigger because with Linus' suggestion the UP kernel uses
<BR>
spinlocks to make the kernel fully preemptible - we will be able to
<BR>
reschedule the kernel in a preemptive way at any point that does not hold
<BR>
any spinlock (or lock). The SMP kernel would still reschedules in a
<BR>
'cooperative' way, because spinlocks would not have this 'collect total
<BR>
current spinlock count' property (and thus preempting kernel space would
<BR>
not be allowed).
<BR>
<P>IMPORTANT: to avoid any misunderstanding, this does not make the SMP
<BR>
kernel worse in any way than it is right now. But this would be a
<BR>
(dramatic wrt. latencies!) 'UP-only' improvement, which _feels_ wrong, to
<BR>
me at least.
<BR>
<P><P>meanwhile i've measured the cost of doing per-process counters on SMP, and
<BR>
it's not that bad as i thought. The following code sequence:
<BR>
<P>void dummy (void)
<BR>
{
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;atomic_inc_local(&amp;current-&gt;may_preempt);
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;atomic_inc_local(&amp;current-&gt;may_preempt);
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;atomic_dec_local(&amp;current-&gt;may_preempt);
<BR>
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;atomic_dec_local(&amp;current-&gt;may_preempt);
<BR>
}
<BR>
<P>simulates a typical scenario, two spinlocks taken and released in a
<BR>
function, with good 'synergy' effects. This translates into:
<BR>
<P>&nbsp;dd4:   b8 00 e0 ff ff          movl   $0xffffe000,%eax
<BR>
&nbsp;dd9:   21 e0                   andl   %esp,%eax
<BR>
&nbsp;ddb:   ff 40 18                incl   0x18(%eax)
<BR>
&nbsp;dde:   ff 40 18                incl   0x18(%eax)
<BR>
&nbsp;de1:   ff 48 18                decl   0x18(%eax)
<BR>
&nbsp;de4:   ff 48 18                decl   0x18(%eax)
<BR>
<P>so there is an offset cost of 2 instructions (7 bytes) to load current
<BR>
(which in i guess 30-40% of the cases is already loaded), and 1
<BR>
instruction (3 bytes) to increase/decrease. If register pressure is higher
<BR>
and 'current' is not used near to where the spinlocks are used then the
<BR>
cost is higher. But this has less icache footprint than doing a dumb
<BR>
global memory counter for example, which has a 6 bytes cost per inc/dec:
<BR>
<P>&nbsp;de8:   ff 05 00 00 00 00       incl   0x0
<BR>
&nbsp;dee:   ff 05 00 00 00 00       incl   0x0
<BR>
&nbsp;df4:   ff 0d 00 00 00 00       decl   0x0
<BR>
&nbsp;dfa:   ff 0d 00 00 00 00       decl   0x0
<BR>
<P>of course it has more footprint than 0 instructions [ == the current SMP
<BR>
kernel].
<BR>
<P>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Ingo
<BR>
<P><!-- body="end" -->
<HR>
<P>
<UL>
<!-- next="start" -->
<LI><STRONG>Next message:</STRONG> <A HREF="0080.html">Andrea Arcangeli: "Re: [patch] preemptive kernel, preemptive-2.3.52-A7"</A>
<LI><STRONG>Previous message:</STRONG> <A HREF="0078.html">Jamie Lokier: "Re: new IRQ scalability changes in 2.3.48"</A>
<LI><STRONG>In reply to:</STRONG> <A HREF="0098.html">yodaiken@fsmlabs.com: "Re: new IRQ scalability changes in 2.3.48"</A>
<!-- nextthread="start" -->
<LI><STRONG>Next in thread:</STRONG> <A HREF="0091.html">yodaiken@fsmlabs.com: "Re: new IRQ scalability changes in 2.3.48"</A>
<LI><STRONG>Reply:</STRONG> <A HREF="0098.html">Ingo Molnar: "Re: new IRQ scalability changes in 2.3.48"</A>
<!-- reply="end" -->
</UL>
<!-- trailer="footer" -->
<HR>
<P>
<SMALL>
<EM>
This archive was generated by <A HREF="http://www.www.fts.frontec.se/~dast/hypermail/">hypermail 2a22</A> 
: <EM>Sat Apr 01 2000 - 04:15:03 PST</EM>
</EM>
</SMALL>
</BODY>
</HTML>
